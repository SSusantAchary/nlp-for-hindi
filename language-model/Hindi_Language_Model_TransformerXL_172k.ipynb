{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.0.57', '1.1.0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai, torch\n",
    "fastai.__version__ , torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 14 23:10:26 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.116                Driver Version: 390.116                   |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 61%   66C    P0    77W / 250W |    235MiB / 11177MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1039      G   /usr/lib/xorg/Xorg                           110MiB |\r\n",
      "|    0      2165      G   compiz                                       113MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gaurav/PycharmProjects/nlp-for-hindi/language-model\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/gaurav/PycharmProjects/nlp-for-hindi/language-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_files, test_files = train_test_split(files, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_files), len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str(train_files[0]).split('/')[-1][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing dataset for fastai\n",
    "# for file in train_files:\n",
    "#     with open(file, 'rb') as f:\n",
    "#         text = pickle.load(f)\n",
    "#     with open(path/'hindi_transformer'/'train'/(str(file).split('/')[-1][:-4]+'.txt'), \"w\") as text_file:\n",
    "#         text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in test_files:\n",
    "#     with open(file, 'rb') as f:\n",
    "#         text = pickle.load(f)\n",
    "#     with open(path/'hindi_transformer'/'valid'/(str(file).split('/')[-1][:-4]+'.txt'), \"w\") as text_file:\n",
    "#         text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from inltk.tokenizer import HindiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HindiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/gaurav/PycharmProjects/nlp-for-hindi/language-model')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HindiTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str):\n",
    "        self.lang = lang\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(str(path/\"../tokenizer/hindi_lm_large.model\"))\n",
    "        \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return self.sp.EncodeAsPieces(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(path/\"../tokenizer/hindi_lm_large.model\"))\n",
    "itos = [sp.IdToPiece(int(i)) for i in range(30000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '▁के',\n",
       " '।',\n",
       " '▁में',\n",
       " '▁है',\n",
       " ',',\n",
       " '▁की',\n",
       " '▁',\n",
       " '▁और',\n",
       " '▁से',\n",
       " '▁का',\n",
       " '▁को',\n",
       " '▁हैं',\n",
       " '▁एक',\n",
       " '▁पर',\n",
       " '.',\n",
       " '-',\n",
       " '▁ने']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30,000 is the vocab size that we chose in sentencepiece\n",
    "hindi_vocab = Vocab(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tok_func=HindiTokenizer, lang='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxfld',\n",
       " 'xxmaj',\n",
       " 'xxup',\n",
       " 'xxrep',\n",
       " 'xxwrep']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/gaurav/PycharmProjects/nlp-for-hindi/language-model')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_folder(path=path/'hindi_transformer', tokenizer=tokenizer, vocab=hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>▁जे स्टन ▁जेम्स ▁बॉण्ड ▁1953 ▁में ▁अंग्रेज़ ▁लेखक ▁इयान ▁फ़्लेम िंग ▁द्वारा ▁रचित ▁एक ▁काल्पनिक ▁पात्र ▁है । ▁007 ▁के ▁गुप्त ▁नाम ▁से ▁प्रसिद्ध ▁यह ▁एजेंट ▁फ़्लेम िंग ▁की ▁बारह ▁पुस्तकों ▁व ▁दो ▁लघुकथा ओं ▁में ▁मौजूद ▁है । ▁1964 ▁में ▁फ़्लेम िंग ▁की ▁मृत्यु ▁के ▁पश्चात ▁छः ▁अन्य ▁लेखकों ▁ने ▁बॉण्ड ▁की ▁आधि कृत ▁पुस्तकें ▁लिखी ▁हैं , ▁जिनमें ▁किंग्स ले ▁ऐ मिस , ▁क्रिस्टोफ़र ▁वुड्स , ▁जॉन ▁गार्ड</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁जो ▁क ल्हण ▁द्वारा ▁12 वीं ▁शताब्दी ▁ई . ▁में ▁लिखा ▁गया ▁था । ▁तब ▁तक ▁यहां ▁पूर्ण ▁हिन्दू ▁राज्य ▁रहा ▁था । यह ▁अशोक ▁महान ▁के ▁साम्राज्य ▁का ▁हिस्सा ▁भी ▁रहा । ▁लगभग ▁तीसरी ▁शताब्दी ▁में ▁अशोक ▁का ▁शासन ▁रहा ▁था । ▁तभी ▁यहां ▁बौद्ध ▁धर्म ▁का ▁आगमन ▁हुआ , ▁जो ▁आगे ▁चलकर ▁कुषाण ों ▁के ▁अधीन ▁सम ृ ध्द ▁हुआ ▁था । उ ज्ज ैन ▁के ▁महाराज ▁विक्रमादित्य</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>▁पाकिस्तान ▁शामिल ▁हैं ▁जबकि ▁भारी ▁मात्रा ▁में ▁कैन ोला ▁ऑयल ▁और ▁मील ▁संयुक्त ▁राज्य ▁अमेरिका ▁जाता ▁है ▁और ▁इसकी ▁छोटी ▁मात्रा एं ▁मैक्सिको , ▁चीन ▁और ▁यूरोप ▁में ▁भेज ▁दी ▁जाती ▁हैं । ▁वर्ष ▁2002 - 2003 ▁के ▁मौसम ▁में ▁दुनिया ▁भर ▁में ▁लगभग ▁14 ▁मिलियन ▁मैट्रिक ▁टन ▁रे प सीड ▁ऑयल ▁का ▁उत्पादन ▁हुआ ▁था । ▁कैन ोला ▁को ▁पारंपरिक ▁पौध ▁प्रजनन ▁के ▁जरिये ▁रे प सीड ▁से ▁विकसित</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>▁फिल्मों ▁में ▁दिखी ं । ▁पहले ▁अभिषेक ▁कपूर ▁की ▁फि तूर ▁में ▁जो ▁चार्ल्स ▁डिकेंस ▁के ▁उपन्यास ▁ग्रेट ▁एक्स्प ेक्ट ै शन ▁पर ▁आधारित ▁थी । ▁फिल्म ▁में ▁आदित्य ▁रॉय ▁कपूर ▁और ▁तब ु ▁भी ▁थे । ▁बाद ▁में ▁बार ▁बार ▁देखो ▁में ▁वह ▁सिद्धार्थ ▁मल्होत्रा ▁के ▁साथ ▁नज़र ▁आई । ▁दोनों ▁ही ▁फिल्म ▁सफल ▁नहीं ▁रही । ▁हिन्दुस्तान ▁के ▁विशाल ▁ठाकुर ▁ने ▁लिखा : ▁\" बार ▁बार ▁देखो ▁में</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁नहीं ▁आते ▁हैं , ▁जबकि ▁मु वत् ता ▁के ▁सभी ▁हदीस ▁अन्य ▁सही ह ▁किताबों ▁में ▁शामिल ▁हैं । ▁सुन्नी ▁मुस्लिम ▁छह ▁प्रमुख ▁हदीस ▁संग्रह ों ▁को ▁उनके ▁सबसे ▁महत्वपूर्ण ▁मानते ▁हैं , ▁हालांकि ▁प्रामाणिकता ▁का ▁क्रम ▁मज़ ह ब ों ▁के ▁बीच ▁भिन्न ▁होता ▁है ▁इब्न ▁हज र ▁के ▁अनुसार , ▁पहले ▁दो , ▁जिसे ▁आमतौर ▁पर ▁दो ▁सह हि ह ▁के ▁रूप ▁में ▁जाना ▁जाता ▁है , ▁उनकी</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "??language_model_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lm.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, TransformerXL, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): TransformerXL(\n",
       "    (encoder): Embedding(30000, 410)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.1)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=30000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.074192</td>\n",
       "      <td>5.015274</td>\n",
       "      <td>0.260868</td>\n",
       "      <td>1:25:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.412691</td>\n",
       "      <td>4.317023</td>\n",
       "      <td>0.317931</td>\n",
       "      <td>1:25:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.188562</td>\n",
       "      <td>4.081395</td>\n",
       "      <td>0.333240</td>\n",
       "      <td>1:25:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.096096</td>\n",
       "      <td>4.047993</td>\n",
       "      <td>0.331070</td>\n",
       "      <td>1:26:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.091798</td>\n",
       "      <td>4.032840</td>\n",
       "      <td>0.328847</td>\n",
       "      <td>1:26:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.044764</td>\n",
       "      <td>4.007892</td>\n",
       "      <td>0.329269</td>\n",
       "      <td>1:25:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.046127</td>\n",
       "      <td>3.945281</td>\n",
       "      <td>0.335464</td>\n",
       "      <td>1:25:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.919419</td>\n",
       "      <td>3.886997</td>\n",
       "      <td>0.340124</td>\n",
       "      <td>1:26:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.838495</td>\n",
       "      <td>3.820855</td>\n",
       "      <td>0.347243</td>\n",
       "      <td>1:26:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.836525</td>\n",
       "      <td>3.766215</td>\n",
       "      <td>0.352727</td>\n",
       "      <td>1:26:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.864718</td>\n",
       "      <td>3.716155</td>\n",
       "      <td>0.358196</td>\n",
       "      <td>1:26:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.713393</td>\n",
       "      <td>3.625276</td>\n",
       "      <td>0.368409</td>\n",
       "      <td>1:26:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.629655</td>\n",
       "      <td>3.567900</td>\n",
       "      <td>0.375502</td>\n",
       "      <td>1:26:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.661837</td>\n",
       "      <td>3.490401</td>\n",
       "      <td>0.384952</td>\n",
       "      <td>1:26:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.486028</td>\n",
       "      <td>3.425163</td>\n",
       "      <td>0.393710</td>\n",
       "      <td>1:26:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.467291</td>\n",
       "      <td>3.363544</td>\n",
       "      <td>0.402195</td>\n",
       "      <td>1:26:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.380324</td>\n",
       "      <td>3.318638</td>\n",
       "      <td>0.408734</td>\n",
       "      <td>1:26:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.364862</td>\n",
       "      <td>3.282482</td>\n",
       "      <td>0.414005</td>\n",
       "      <td>1:26:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.335473</td>\n",
       "      <td>3.266114</td>\n",
       "      <td>0.416475</td>\n",
       "      <td>1:26:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.225191</td>\n",
       "      <td>3.261883</td>\n",
       "      <td>0.416972</td>\n",
       "      <td>1:26:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with accuracy value: 0.2608684301376343.\n",
      "Better model found at epoch 1 with accuracy value: 0.31793108582496643.\n",
      "Better model found at epoch 2 with accuracy value: 0.3332400619983673.\n",
      "Better model found at epoch 6 with accuracy value: 0.3354644179344177.\n",
      "Better model found at epoch 7 with accuracy value: 0.34012407064437866.\n",
      "Better model found at epoch 8 with accuracy value: 0.34724289178848267.\n",
      "Better model found at epoch 9 with accuracy value: 0.35272735357284546.\n",
      "Better model found at epoch 10 with accuracy value: 0.35819557309150696.\n",
      "Better model found at epoch 11 with accuracy value: 0.36840856075286865.\n",
      "Better model found at epoch 12 with accuracy value: 0.37550199031829834.\n",
      "Better model found at epoch 13 with accuracy value: 0.3849523663520813.\n",
      "Better model found at epoch 14 with accuracy value: 0.3937096893787384.\n",
      "Better model found at epoch 15 with accuracy value: 0.40219518542289734.\n",
      "Better model found at epoch 16 with accuracy value: 0.4087342619895935.\n",
      "Better model found at epoch 17 with accuracy value: 0.4140048027038574.\n",
      "Better model found at epoch 18 with accuracy value: 0.4164750874042511.\n",
      "Better model found at epoch 19 with accuracy value: 0.41697242856025696.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, 1e-3, moms=(0.8,0.7), callbacks=[callbacks.SaveModelCallback(learn, every='improvement', monitor='accuracy', name='model')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"जिसके लिये उन्हें \"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "जिसके लिये उन्हें  ▁बर्मी ▁अथवा ▁चो शि यु ▁भी ▁कहा ▁जाता ▁है । ▁x x bo s ▁साल ▁2007 ▁से ▁अब ▁तक ▁48 ▁मैचों ▁की ▁58 ▁सीरीज ▁हुई ▁हैं । ▁उसमें ▁श्रीलंका ▁ने ▁5 ▁सीरीज ▁जीते ▁हैं । ▁साल ▁1979 ▁से ▁अब ▁तक\n",
      "जिसके लिये उन्हें  ▁वर्ष ▁2010 ▁में ▁मरणोपरांत ▁साहित्य ▁अकादमी ▁पुरस्कार ▁से ▁सम्मानित ▁किया ▁गया । ▁x x bo s ▁ डो ग , खेत ▁खर सी या ▁मण्डल ▁में ▁भारत ▁के ▁छत्तीसगढ़ ▁राज्य ▁के ▁अन्तर्गत ▁रायगढ़ ▁जिले ▁का ▁एक ▁गाँव ▁है । ▁x\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.9) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.09863463173677"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.261883)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults.device = torch.device('cpu')\n",
    "learn.model.eval()\n",
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/gaurav/PycharmProjects/nlp-for-hindi/language-model')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = load_learner(path / 'HindiDataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_model(learn.model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 410])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.state_dict()['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = encoder.state_dict()['encoder.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 410)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('transformer3_embeddings.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.099972</td>\n",
       "      <td>-0.164632</td>\n",
       "      <td>-0.261503</td>\n",
       "      <td>0.113243</td>\n",
       "      <td>-0.010116</td>\n",
       "      <td>0.008201</td>\n",
       "      <td>-0.083398</td>\n",
       "      <td>-0.439391</td>\n",
       "      <td>-0.210696</td>\n",
       "      <td>-0.042600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.437657</td>\n",
       "      <td>0.091170</td>\n",
       "      <td>-0.101380</td>\n",
       "      <td>0.095993</td>\n",
       "      <td>0.254158</td>\n",
       "      <td>-0.199073</td>\n",
       "      <td>0.176711</td>\n",
       "      <td>-0.015598</td>\n",
       "      <td>-0.083239</td>\n",
       "      <td>0.054953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.050656</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.005276</td>\n",
       "      <td>0.056509</td>\n",
       "      <td>0.063262</td>\n",
       "      <td>-0.037668</td>\n",
       "      <td>-0.175472</td>\n",
       "      <td>-0.001132</td>\n",
       "      <td>-0.134797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255351</td>\n",
       "      <td>-0.030477</td>\n",
       "      <td>-0.037093</td>\n",
       "      <td>0.019064</td>\n",
       "      <td>-0.031267</td>\n",
       "      <td>-0.072239</td>\n",
       "      <td>0.029152</td>\n",
       "      <td>-0.178726</td>\n",
       "      <td>-0.167360</td>\n",
       "      <td>0.058221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.052035</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>0.022010</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.058504</td>\n",
       "      <td>0.062911</td>\n",
       "      <td>-0.037709</td>\n",
       "      <td>-0.173654</td>\n",
       "      <td>-0.002538</td>\n",
       "      <td>-0.134034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255153</td>\n",
       "      <td>-0.030363</td>\n",
       "      <td>-0.038712</td>\n",
       "      <td>0.020665</td>\n",
       "      <td>-0.031761</td>\n",
       "      <td>-0.071156</td>\n",
       "      <td>0.030784</td>\n",
       "      <td>-0.180084</td>\n",
       "      <td>-0.167233</td>\n",
       "      <td>0.058573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.234248</td>\n",
       "      <td>0.060820</td>\n",
       "      <td>0.207638</td>\n",
       "      <td>0.096909</td>\n",
       "      <td>0.204999</td>\n",
       "      <td>-0.002955</td>\n",
       "      <td>-0.095316</td>\n",
       "      <td>-0.506015</td>\n",
       "      <td>0.017120</td>\n",
       "      <td>-0.110036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216290</td>\n",
       "      <td>0.067957</td>\n",
       "      <td>-0.034987</td>\n",
       "      <td>0.040247</td>\n",
       "      <td>0.227386</td>\n",
       "      <td>-0.385344</td>\n",
       "      <td>0.099810</td>\n",
       "      <td>-0.569107</td>\n",
       "      <td>-0.130740</td>\n",
       "      <td>0.122291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.317653</td>\n",
       "      <td>0.194561</td>\n",
       "      <td>-0.275492</td>\n",
       "      <td>0.176436</td>\n",
       "      <td>0.224074</td>\n",
       "      <td>-0.246735</td>\n",
       "      <td>0.113164</td>\n",
       "      <td>-0.212135</td>\n",
       "      <td>0.026025</td>\n",
       "      <td>-0.478695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653744</td>\n",
       "      <td>-0.218545</td>\n",
       "      <td>-0.187402</td>\n",
       "      <td>0.219913</td>\n",
       "      <td>0.126067</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>-0.429442</td>\n",
       "      <td>0.010987</td>\n",
       "      <td>0.399669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.099972 -0.164632 -0.261503  0.113243 -0.010116  0.008201 -0.083398   \n",
       "1  0.050656  0.013242  0.023480  0.005276  0.056509  0.063262 -0.037668   \n",
       "2  0.052035  0.012945  0.022010  0.004850  0.058504  0.062911 -0.037709   \n",
       "3 -0.234248  0.060820  0.207638  0.096909  0.204999 -0.002955 -0.095316   \n",
       "4  0.317653  0.194561 -0.275492  0.176436  0.224074 -0.246735  0.113164   \n",
       "\n",
       "        7         8         9    ...       400       401       402       403  \\\n",
       "0 -0.439391 -0.210696 -0.042600  ... -0.437657  0.091170 -0.101380  0.095993   \n",
       "1 -0.175472 -0.001132 -0.134797  ... -0.255351 -0.030477 -0.037093  0.019064   \n",
       "2 -0.173654 -0.002538 -0.134034  ... -0.255153 -0.030363 -0.038712  0.020665   \n",
       "3 -0.506015  0.017120 -0.110036  ... -0.216290  0.067957 -0.034987  0.040247   \n",
       "4 -0.212135  0.026025 -0.478695  ... -0.653744 -0.218545 -0.187402  0.219913   \n",
       "\n",
       "        404       405       406       407       408       409  \n",
       "0  0.254158 -0.199073  0.176711 -0.015598 -0.083239  0.054953  \n",
       "1 -0.031267 -0.072239  0.029152 -0.178726 -0.167360  0.058221  \n",
       "2 -0.031761 -0.071156  0.030784 -0.180084 -0.167233  0.058573  \n",
       "3  0.227386 -0.385344  0.099810 -0.569107 -0.130740  0.122291  \n",
       "4  0.126067 -0.025123  0.033977 -0.429442  0.010987  0.399669  \n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 410)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>▁के</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  <unk>\n",
       "1    <s>\n",
       "2   </s>\n",
       "3    ▁के\n",
       "4      ।"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('transformer3_embeddings_metadata.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.0656e-02,  1.3242e-02,  2.3480e-02,  5.2757e-03,  5.6509e-02,\n",
       "         6.3262e-02, -3.7668e-02, -1.7547e-01, -1.1316e-03, -1.3480e-01,\n",
       "        -7.2113e-03, -1.8429e-02,  6.4644e-02,  1.2132e-01, -2.8948e-02,\n",
       "        -3.2840e-02,  6.2042e-03, -2.8391e-02,  1.0298e-02,  3.1041e-03,\n",
       "         7.5052e-02,  5.8731e-02,  2.2466e-02, -1.5483e-01,  7.0984e-02,\n",
       "        -1.7122e-02,  4.4589e-02,  3.4579e-02,  1.9023e-02,  2.5846e-02,\n",
       "         2.4408e-02, -5.1587e-02,  5.7413e-03, -8.4907e-03, -6.2721e-02,\n",
       "         3.4298e-02, -1.0657e-03, -2.1233e-03,  1.7872e-02, -4.6330e-02,\n",
       "        -9.7442e-02, -6.7839e-02,  1.4847e-01, -1.1888e-01, -8.5665e-02,\n",
       "        -2.7119e-02,  4.4531e-03, -2.2153e-03, -4.7003e-02,  2.1623e-01,\n",
       "         5.0883e-03,  9.1632e-04,  7.6505e-02, -1.0731e-01, -1.2035e-01,\n",
       "        -6.8442e-02, -1.4023e-02,  8.4317e-02,  2.1350e-03,  7.3595e-03,\n",
       "         1.1860e-04,  1.5694e-02,  3.8656e-03,  8.9918e-03, -4.1664e-02,\n",
       "         4.9456e-02,  3.7580e-02, -1.8443e-02,  3.6612e-02,  1.1217e-01,\n",
       "         2.2807e-01, -6.8999e-02,  5.0330e-02, -5.7284e-02, -8.3027e-02,\n",
       "         7.8370e-03, -1.0025e-02, -6.3295e-02,  2.8806e-02, -5.1965e-02,\n",
       "         6.3856e-02, -4.6325e-02,  5.7598e-02, -9.5437e-03, -2.0365e-02,\n",
       "         6.6406e-02,  1.1256e-02,  2.1162e-02, -6.2357e-02,  3.7167e-02,\n",
       "        -8.4535e-02,  2.4254e-02, -3.9129e-02, -6.4247e-02,  3.1853e-02,\n",
       "        -2.7636e-02, -1.0975e-02,  2.4096e-02,  2.0454e-02, -2.4301e-02,\n",
       "         1.2506e-02,  6.3875e-02,  7.1419e-02,  3.4079e-02,  6.8084e-02,\n",
       "        -2.8068e-02, -6.3978e-02, -7.1514e-03, -3.2478e-02,  2.5371e-02,\n",
       "        -6.2349e-02, -2.2309e-02,  3.5836e-05, -1.4245e-03, -1.1740e-02,\n",
       "         1.0618e-02, -1.0656e-02, -5.5328e-02, -6.2446e-02, -1.2068e-01,\n",
       "        -1.8177e-01, -1.1060e-01,  8.0311e-02,  3.0907e-02, -1.6772e-03,\n",
       "        -2.0087e-02, -4.7168e-02,  1.0695e-02, -4.5706e-02, -3.1445e-03,\n",
       "        -9.7472e-02, -1.8877e-01,  3.5175e-01, -5.2590e-02, -4.8911e-02,\n",
       "        -1.5161e-01,  1.7336e-01, -4.6395e-03,  1.8659e-02, -1.1064e-01,\n",
       "        -2.4139e-01,  1.0921e-02,  1.6398e-02, -5.6421e-02,  3.5258e-02,\n",
       "        -2.4469e-02,  1.0934e-01, -2.2726e-02, -5.9306e-03, -1.2413e-05,\n",
       "        -1.5771e-02,  8.0107e-02, -1.6609e-02,  2.2906e-01,  3.6846e-02,\n",
       "         1.4692e-02, -3.7585e-02,  3.8085e-01,  3.4497e-02, -1.6139e-01,\n",
       "         9.3505e-02, -2.6396e-03, -5.0270e-02, -7.3960e-02,  5.1344e-03,\n",
       "        -6.7529e-02, -2.0915e-01, -1.2576e-01, -2.1735e-04, -1.9994e-03,\n",
       "        -1.9039e-02, -8.7684e-02,  1.9307e-02,  3.2286e-02, -9.2228e-03,\n",
       "        -1.1982e-01,  5.4506e-02,  5.1111e-02, -1.0419e-02,  4.4231e-02,\n",
       "         2.8039e-02,  3.4680e-02,  1.2178e-01,  1.5167e-02, -1.1005e-01,\n",
       "        -3.1951e-02, -3.8300e-03,  2.2859e-02,  3.8487e-02,  2.5735e-02,\n",
       "         1.0575e-02, -4.7987e-02, -3.0104e-02, -8.5111e-02,  8.6246e-02,\n",
       "         1.6918e-02,  7.5921e-02,  4.4339e-03, -6.6624e-02,  3.0378e-02,\n",
       "         5.2167e-03,  3.2058e-01,  1.3249e-01,  4.6685e-02, -2.8499e-03,\n",
       "         9.4871e-02, -1.1373e-01,  1.5399e-02, -7.2755e-02,  2.9355e-02,\n",
       "        -2.8283e-02, -6.7321e-02,  2.0032e-01,  9.9979e-02,  1.4967e-01,\n",
       "        -3.7655e-03,  2.1711e-01, -3.7825e-02,  3.4148e-02, -5.2607e-02,\n",
       "        -2.1382e-02, -1.4589e-02,  2.7698e-01, -6.0767e-02, -9.0150e-03,\n",
       "         2.4664e-02,  7.9007e-02, -7.2796e-02,  4.5905e-02, -3.9751e-02,\n",
       "         2.1217e-02,  2.3667e-02,  5.8100e-02,  6.7359e-02, -9.6938e-03,\n",
       "        -2.8555e-02,  8.4434e-03, -1.4891e-01,  1.0388e-01,  1.7537e-02,\n",
       "         1.2317e-01,  9.0843e-02,  3.2604e-02, -6.9219e-02,  1.7922e-02,\n",
       "        -1.6663e-01,  1.3265e-01, -1.1027e-01,  1.0015e-02, -1.3478e-02,\n",
       "        -1.1931e-01, -6.4607e-02,  2.0124e-02,  1.5606e-02,  2.2370e-02,\n",
       "         3.2464e-02,  8.9678e-02, -5.6609e-02,  2.0690e-02, -1.1040e-01,\n",
       "        -8.5594e-03,  1.4072e-02, -2.0003e-02, -1.2807e-02,  5.2172e-02,\n",
       "        -8.8147e-02,  1.0300e-01, -5.0730e-02, -6.8871e-02,  2.1444e-02,\n",
       "         4.0150e-04,  2.3980e-02, -2.2633e-02,  1.6460e-02,  2.5711e-02,\n",
       "         3.0610e-02,  3.7705e-02,  6.9931e-02, -4.3815e-02,  2.4061e-02,\n",
       "         1.6624e-01, -1.3245e-02,  1.0356e-01,  2.4516e-02, -3.3962e-02,\n",
       "         7.3025e-03,  3.5299e-02, -6.5993e-02, -2.7829e-02,  1.6398e-02,\n",
       "         2.9198e-01, -5.5268e-02,  5.4428e-02, -1.4158e-01, -1.7320e-01,\n",
       "         1.4138e-02,  4.5654e-02,  1.3337e-02, -5.1941e-02, -9.8113e-03,\n",
       "        -1.9837e-01, -1.9948e-02,  3.9828e-02, -3.2904e-02, -1.9715e-01,\n",
       "         1.8128e-02,  1.1107e-02,  4.5440e-02, -8.2876e-02, -1.9173e-02,\n",
       "         4.6941e-02,  9.7118e-02, -2.2928e-01,  3.6161e-02,  3.4942e-02,\n",
       "        -9.1640e-03, -2.4777e-02, -6.5125e-03,  1.9604e-02, -1.3599e-01,\n",
       "        -1.1477e-01, -1.1444e-01,  1.7434e-02,  8.6008e-02,  3.7520e-02,\n",
       "        -8.4861e-02, -1.2059e-01,  9.0891e-02,  1.0568e-01,  1.0005e-01,\n",
       "         1.7307e-01,  5.1416e-02, -2.6329e-02,  2.1981e-02, -1.6127e-01,\n",
       "        -5.0700e-02, -4.2929e-03, -2.2049e-02,  1.5071e-01,  2.9565e-02,\n",
       "         2.0342e-02,  7.9813e-03, -6.0186e-03, -1.3067e-01, -6.2830e-02,\n",
       "         3.2539e-03, -1.3242e-01,  2.6112e-01, -2.0987e-01, -6.1793e-02,\n",
       "         3.9484e-05,  1.4935e-01, -1.9232e-02,  2.4302e-02, -1.7242e-02,\n",
       "         2.6441e-03, -1.1132e-02, -2.3149e-01,  8.5944e-03,  4.8183e-02,\n",
       "        -2.9147e-03, -9.7672e-02,  1.8745e-02,  1.3182e-01,  1.3713e-02,\n",
       "         1.0864e-01, -1.1162e-01,  3.1873e-02, -8.0539e-02, -8.0032e-02,\n",
       "        -1.2489e-01,  3.6867e-03, -9.5043e-03,  1.6820e-02, -2.1751e-02,\n",
       "         3.4390e-02,  2.7523e-02, -1.7693e-02, -7.1201e-03, -2.1822e-02,\n",
       "        -4.0029e-02, -2.7987e-02, -1.8678e-01, -4.3399e-02, -1.4886e-01,\n",
       "         3.4930e-02, -1.7412e-02,  6.7505e-02, -6.8466e-02, -1.0606e-02,\n",
       "        -3.1098e-02,  3.8319e-02, -2.3795e-02,  1.2339e-01,  5.5776e-02,\n",
       "         8.8078e-02, -2.6969e-01,  2.1350e-02, -2.4553e-03, -9.6651e-02,\n",
       "        -2.5535e-01, -3.0477e-02, -3.7093e-02,  1.9064e-02, -3.1267e-02,\n",
       "        -7.2239e-02,  2.9152e-02, -1.7873e-01, -1.6736e-01,  5.8221e-02],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.state_dict()['encoder.weight'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
